{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/feliciakiani/PolitikPedia/blob/main/Machine%20Learning/ETL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7UJEVfn9BAi"
      },
      "source": [
        "Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PySastrawi in /home/c003bsy3376/.local/lib/python3.9/site-packages (1.2.0)\n",
            "Requirement already satisfied: pandas in /home/c003bsy3376/.local/lib/python3.9/site-packages (2.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.9/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/c003bsy3376/.local/lib/python3.9/site-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /home/c003bsy3376/.local/lib/python3.9/site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.9/dist-packages (from pandas) (1.26.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: nltk in /home/c003bsy3376/.local/lib/python3.9/site-packages (3.8.1)\n",
            "Requirement already satisfied: joblib in /home/c003bsy3376/.local/lib/python3.9/site-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: tqdm in /home/c003bsy3376/.local/lib/python3.9/site-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/c003bsy3376/.local/lib/python3.9/site-packages (from nltk) (2023.10.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: mysql-connector-python in /home/c003bsy3376/.local/lib/python3.9/site-packages (8.2.0)\n",
            "Requirement already satisfied: protobuf<=4.21.12,>=4.21.1 in /home/c003bsy3376/.local/lib/python3.9/site-packages (from mysql-connector-python) (4.21.12)\n",
            "Requirement already satisfied: TextBlob in /home/c003bsy3376/.local/lib/python3.9/site-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /home/c003bsy3376/.local/lib/python3.9/site-packages (from TextBlob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk>=3.1->TextBlob) (8.1.7)\n",
            "Requirement already satisfied: tqdm in /home/c003bsy3376/.local/lib/python3.9/site-packages (from nltk>=3.1->TextBlob) (4.66.1)\n",
            "Requirement already satisfied: joblib in /home/c003bsy3376/.local/lib/python3.9/site-packages (from nltk>=3.1->TextBlob) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/c003bsy3376/.local/lib/python3.9/site-packages (from nltk>=3.1->TextBlob) (2023.10.3)\n",
            "Requirement already satisfied: python-dotenv in /home/c003bsy3376/.local/lib/python3.9/site-packages (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install PySastrawi\n",
        "\n",
        "!pip install pandas\n",
        "!pip install nltk\n",
        "!pip install mysql-connector-python\n",
        "!pip install TextBlob\n",
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxFIEMCaA9G_",
        "outputId": "63b9d13b-f42b-44af-c9a3-4f152508b20c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-12-15 07:10:00.295023: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-12-15 07:10:00.350491: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-15 07:10:00.350539: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-15 07:10:00.352330: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-15 07:10:00.361334: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-12-15 07:10:00.362012: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-15 07:10:01.500153: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import re\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "import mysql.connector\n",
        "import os\n",
        "from textblob import TextBlob\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "# install .env\n",
        "%load_ext dotenv\n",
        "%dotenv Backend/.env\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.models import load_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hgh4CYgkBIbr",
        "outputId": "2a3b5037-7401-4cb0-8e7a-e359b06be8fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/c003bsy3376/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Download the NLTK stop words dataset (if not already downloaded)\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Get the list of Indonesian stop words\n",
        "stop_words_indonesian = set(stopwords.words('indonesian'))\n",
        "id_stopword_dict = pd.DataFrame({'stop_word': list(stop_words_indonesian)})\n",
        "\n",
        "alay_dict = pd.read_csv('../Machine Learning/Data/new_kamusalay.csv', encoding='latin-1', header=None)\n",
        "alay_dict = alay_dict.rename(columns={0: 'original',\n",
        "                                      1: 'replacement'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Fjrjy5dR9ARs"
      },
      "outputs": [],
      "source": [
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "\n",
        "def lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "def remove_unnecessary_char(text):\n",
        "    # Remove every '\\n'\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "\n",
        "    # Remove every retweet symbol\n",
        "    text = re.sub(r'rt', ' ', text)\n",
        "\n",
        "    # Remove every username\n",
        "    text = re.sub(r'user', ' ', text)\n",
        "\n",
        "    # Remove every URL\n",
        "    text = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', ' ', text)\n",
        "\n",
        "    # Remove all emojis (Unicode characters)\n",
        "    text = re.sub(r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002702-\\U000027B0\\U000024C2-\\U0001F251]', ' ', text)\n",
        "\n",
        "   # Remove all hexadecimal representations of UTF-8 encoded characters\n",
        "    text = re.sub(r'\\\\x[0-9a-fA-F]{2}', ' ', text)\n",
        "    text = re.sub(r'x[0-9a-fA-F]{2}', ' ', text)  # If the 'x' is not escaped\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'  +', ' ', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def remove_nonaplhanumeric(text):\n",
        "    text = re.sub('[^0-9a-zA-Z]+', ' ', text)\n",
        "    return text\n",
        "\n",
        "alay_dict_map = dict(zip(alay_dict['original'], alay_dict['replacement']))\n",
        "def normalize_alay(text):\n",
        "    return ' '.join([alay_dict_map[word] if word in alay_dict_map else word for word in text.split(' ')])\n",
        "\n",
        "def remove_stopword(text):\n",
        "    stop_words = set(id_stopword_dict['stop_word'])\n",
        "    text = ' '.join(['' if word in stop_words else word for word in text.split(' ')])\n",
        "    text = re.sub('  +', ' ', text)  # Remove extra spaces\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "def stemming(text):\n",
        "    return stemmer.stem(text)\n",
        "\n",
        "def preprocess(text):\n",
        "    text = lowercase(text) # 1\n",
        "    text = remove_nonaplhanumeric(text) # 2\n",
        "    text = remove_unnecessary_char(text) # 2\n",
        "    text = normalize_alay(text) # 3\n",
        "    text = stemming(text) # 4\n",
        "    text = remove_stopword(text) # 5\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        },
        "id": "heQqdoJX7pWK",
        "outputId": "3727e0e4-38e6-4060-f7d1-0b364aa95643"
      },
      "outputs": [],
      "source": [
        "#kodingannya taru diamana ? access modelnya taro dimana gitu\n",
        "\n",
        "# Koneksi ke database Cloud SQL\n",
        "def predict_sentiment(target_comment_id):\n",
        "    conn = mysql.connector.connect(\n",
        "        host=os.getenv(\"DB_HOST\"),\n",
        "        user=os.getenv(\"DB_USERNAME\"),\n",
        "        password=os.getenv(\"DB_PASSWORD\"),\n",
        "        database=os.getenv(\"DB_NAME\")\n",
        "    )\n",
        "\n",
        "    # Buat kursor\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Query data komentar\n",
        "    query = f\"SELECT ID as comment_id, Komentar as komentar_text FROM komentar WHERE ID = {target_comment_id}\"\n",
        "    cursor.execute(query)\n",
        "    comment = cursor.fetchall()\n",
        "\n",
        "    #-------------------------------------------------Text_Classification-------------------------------------------#\n",
        "    #Hyperparameter, jangan diganti\n",
        "    max_length = 56\n",
        "\n",
        "    # Tokenize the sentences\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "    # Read the content of 'sentences.txt' and split it into lines\n",
        "    with open('sentences.txt', 'r', encoding='utf-8') as file:\n",
        "        tokenizer_sentences = file.read().splitlines()\n",
        "    tokenizer.fit_on_texts(tokenizer_sentences)\n",
        "\n",
        "    # Load the best-performing model\n",
        "    model_path = 'Text_Classification.h5'\n",
        "    model = load_model(model_path)\n",
        "    # Check if the comment exists\n",
        "    if comment:\n",
        "        for row in comment:\n",
        "            comment_id, komentar_text = row  # Extracting comment_id and Komentar\n",
        "            print(f\"Comment ID: {comment_id}, Komentar: {komentar_text}\")\n",
        "\n",
        "        # Apply preprocessing\n",
        "        preprocessed_text = preprocess(komentar_text)\n",
        "\n",
        "        # Tokenize and pad the new text\n",
        "        new_sequence = tokenizer.texts_to_sequences([preprocessed_text])\n",
        "        new_padded_sequence = pad_sequences(new_sequence, maxlen=max_length, padding='post')\n",
        "\n",
        "        # Make predictions\n",
        "        prediction = model.predict(new_padded_sequence)\n",
        "\n",
        "        # Extract the scalar value from the NumPy array\n",
        "        confidence = prediction[0, 0]\n",
        "\n",
        "        # Threshold for considering a label as positive\n",
        "        threshold = 0.5\n",
        "\n",
        "        # Interpret prediction\n",
        "        predicted_class = 1 if confidence >= threshold else 0\n",
        "\n",
        "        # Convert confidence to a format that can be handled by format method\n",
        "        confidence_str = '{:.4f}'.format(confidence)\n",
        "\n",
        "        # Return the results\n",
        "        result = {\n",
        "            \"comment_id\": comment_id,\n",
        "            \"predicted_class\": predicted_class,\n",
        "            \"confidence\": confidence_str\n",
        "        }\n",
        "    else:\n",
        "        result = {\"error\": f\"Comment with ID {target_comment_id} not found.\"}\n",
        "\n",
        "    # Close the database connection\n",
        "    cursor.close()\n",
        "    conn.close()\n",
        "\n",
        "    return result   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n",
            "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:8888\n",
            " * Running on http://10.88.0.3:8888\n",
            "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "Comment ID: 10, Komentar: dasar anjing jadi cawapres bisanya cuma janji manis doang\n",
            "1/1 [==============================] - 1s 672ms/step\n",
            "10.88.0.3 - - [15/Dec/2023 07:02:43] \"GET /predict_sentiment?target_comment_id=10 HTTP/1.1\" 200 -\n",
            "Comment ID: 7, Komentar: Ni orang bagus nih\n",
            "1/1 [==============================] - 1s 644ms/step\n",
            "10.88.0.3 - - [15/Dec/2023 07:03:44] \"GET /predict_sentiment?target_comment_id=7 HTTP/1.1\" 200 -\n",
            "10.88.0.3 - - [15/Dec/2023 07:04:03] \"GET /predict_sentiment?target_comment_id=12 HTTP/1.1\" 200 -\n",
            "Comment ID: 8, Komentar: ini yang jadi cawapres itu ya?\n",
            "1/1 [==============================] - 1s 661ms/step\n",
            "10.88.0.3 - - [15/Dec/2023 07:04:26] \"GET /predict_sentiment?target_comment_id=8 HTTP/1.1\" 200 -\n",
            "Comment ID: 10, Komentar: dasar anjing jadi cawapres bisanya cuma janji manis doang\n",
            "1/1 [==============================] - 1s 903ms/step\n",
            "10.88.0.3 - - [15/Dec/2023 07:04:38] \"GET /predict_sentiment?target_comment_id=10 HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "# flask\n",
        "from flask import Flask, jsonify, request\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/predict_sentiment', methods=['GET'])\n",
        "def predict_sentiment_endpoint():\n",
        "    target_comment_id = request.args.get('target_comment_id')\n",
        "    \n",
        "    if target_comment_id is not None:\n",
        "        # Call the predict_sentiment function\n",
        "        result = predict_sentiment(target_comment_id)\n",
        "        return jsonify(result)\n",
        "    else:\n",
        "        return jsonify({\"error\": \"target_comment_id parameter is missing.\"})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(host='0.0.0.0', port=8888)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNdsZze64Hk+3tv2LMLyHMk",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2-final"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}