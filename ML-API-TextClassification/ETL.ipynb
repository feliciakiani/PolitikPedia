{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/feliciakiani/PolitikPedia/blob/main/Machine%20Learning/ETL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7UJEVfn9BAi"
      },
      "source": [
        "Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxFIEMCaA9G_",
        "outputId": "63b9d13b-f42b-44af-c9a3-4f152508b20c"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import re\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "import mysql.connector\n",
        "import os\n",
        "from textblob import TextBlob\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "# install .env\n",
        "%load_ext dotenv\n",
        "%dotenv Backend/.env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hgh4CYgkBIbr",
        "outputId": "2a3b5037-7401-4cb0-8e7a-e359b06be8fc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Felicia\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Download the NLTK stop words dataset (if not already downloaded)\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Get the list of Indonesian stop words\n",
        "stop_words_indonesian = set(stopwords.words('indonesian'))\n",
        "id_stopword_dict = pd.DataFrame({'stop_word': list(stop_words_indonesian)})\n",
        "\n",
        "alay_dict = pd.read_csv('../Machine Learning/Data/new_kamusalay.csv', encoding='latin-1', header=None)\n",
        "alay_dict = alay_dict.rename(columns={0: 'original',\n",
        "                                      1: 'replacement'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fjrjy5dR9ARs"
      },
      "outputs": [],
      "source": [
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "\n",
        "def lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "def remove_unnecessary_char(text):\n",
        "    # Remove every '\\n'\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "\n",
        "    # Remove every retweet symbol\n",
        "    text = re.sub(r'rt', ' ', text)\n",
        "\n",
        "    # Remove every username\n",
        "    text = re.sub(r'user', ' ', text)\n",
        "\n",
        "    # Remove every URL\n",
        "    text = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', ' ', text)\n",
        "\n",
        "    # Remove all emojis (Unicode characters)\n",
        "    text = re.sub(r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002702-\\U000027B0\\U000024C2-\\U0001F251]', ' ', text)\n",
        "\n",
        "   # Remove all hexadecimal representations of UTF-8 encoded characters\n",
        "    text = re.sub(r'\\\\x[0-9a-fA-F]{2}', ' ', text)\n",
        "    text = re.sub(r'x[0-9a-fA-F]{2}', ' ', text)  # If the 'x' is not escaped\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'  +', ' ', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def remove_nonaplhanumeric(text):\n",
        "    text = re.sub('[^0-9a-zA-Z]+', ' ', text)\n",
        "    return text\n",
        "\n",
        "alay_dict_map = dict(zip(alay_dict['original'], alay_dict['replacement']))\n",
        "def normalize_alay(text):\n",
        "    return ' '.join([alay_dict_map[word] if word in alay_dict_map else word for word in text.split(' ')])\n",
        "\n",
        "def remove_stopword(text):\n",
        "    stop_words = set(id_stopword_dict['stop_word'])\n",
        "    text = ' '.join(['' if word in stop_words else word for word in text.split(' ')])\n",
        "    text = re.sub('  +', ' ', text)  # Remove extra spaces\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "def stemming(text):\n",
        "    return stemmer.stem(text)\n",
        "\n",
        "def preprocess(text):\n",
        "    text = lowercase(text) # 1\n",
        "    text = remove_nonaplhanumeric(text) # 2\n",
        "    text = remove_unnecessary_char(text) # 2\n",
        "    text = normalize_alay(text) # 3\n",
        "    text = stemming(text) # 4\n",
        "    text = remove_stopword(text) # 5\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        },
        "id": "heQqdoJX7pWK",
        "outputId": "3727e0e4-38e6-4060-f7d1-0b364aa95643"
      },
      "outputs": [],
      "source": [
        "# Koneksi ke database Cloud SQL\n",
        "def predict_sentiment(user_id):\n",
        "    conn = mysql.connector.connect(\n",
        "        host=os.getenv(\"DB_HOST\"),\n",
        "        user=os.getenv(\"DB_USERNAME\"),\n",
        "        password=os.getenv(\"DB_PASSWORD\"),\n",
        "        database=os.getenv(\"DB_NAME\")\n",
        "    )\n",
        "\n",
        "    # Buat kursor\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Query data komentar\n",
        "    query = f\"SELECT ID as comment_id, Komentar as komentar_text FROM komentar WHERE IDUser = {user_id} ORDER BY TglKomentar DESC LIMIT 1\"\n",
        "    cursor.execute(query)\n",
        "    comment = cursor.fetchall()\n",
        "\n",
        "    #-------------------------------------------------Text_Classification-------------------------------------------#\n",
        "    #Hyperparameter\n",
        "    max_length = 56\n",
        "\n",
        "    # Tokenize the sentences\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "    # Read the content of 'sentences.txt' and split it into lines\n",
        "    with open('sentences.txt', 'r', encoding='utf-8') as file:\n",
        "        tokenizer_sentences = file.read().splitlines()\n",
        "    tokenizer.fit_on_texts(tokenizer_sentences)\n",
        "\n",
        "    # Load the best-performing model\n",
        "    model_path = 'Text_Classification.h5'\n",
        "    model = load_model(model_path)\n",
        "    # Check if the comment exists\n",
        "    if comment:\n",
        "        for row in comment:\n",
        "            comment_id, komentar_text = row  # Extracting comment_id and Komentar\n",
        "            print(f\"Comment ID: {comment_id}, Komentar: {komentar_text}\")\n",
        "\n",
        "        # Apply preprocessing\n",
        "        preprocessed_text = preprocess(komentar_text)\n",
        "\n",
        "        # Tokenize and pad the new text\n",
        "        new_sequence = tokenizer.texts_to_sequences([preprocessed_text])\n",
        "        new_padded_sequence = pad_sequences(new_sequence, maxlen=max_length, padding='post')\n",
        "\n",
        "        # Make predictions\n",
        "        prediction = model.predict(new_padded_sequence)\n",
        "\n",
        "        # Extract the scalar value from the NumPy array\n",
        "        confidence = prediction[0, 0]\n",
        "\n",
        "        # Threshold for considering a label as positive\n",
        "        threshold = 0.5\n",
        "\n",
        "        # Interpret prediction\n",
        "        predicted_class = 1 if confidence >= threshold else 0\n",
        "\n",
        "        # Convert confidence to a format that can be handled by format method\n",
        "        confidence_str = '{:.4f}'.format(confidence)\n",
        "\n",
        "        # Return the results\n",
        "        result = {\n",
        "            \"comment_id\": comment_id,\n",
        "            \"predicted_class\": predicted_class,\n",
        "            \"confidence\": confidence_str\n",
        "        }\n",
        "    else:\n",
        "        result = {\"error\": f\"Comment with ID {user_id} not found.\"}\n",
        "\n",
        "    # Close the database connection\n",
        "    cursor.close()\n",
        "    conn.close()\n",
        "\n",
        "    return result   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://192.168.1.54:5000\n",
            "Press CTRL+C to quit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comment ID: 21, Komentar: goblok pisan anjing kontol\n",
            "1/1 [==============================] - 0s 445ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "192.168.1.54 - - [17/Dec/2023 10:32:05] \"GET /predict_sentiment HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comment ID: 22, Komentar: hah sebenernya tetep kurang bagus sih, tapi dibanding calon lain paling mending\n",
            "1/1 [==============================] - 0s 479ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "192.168.1.54 - - [17/Dec/2023 10:51:47] \"GET /predict_sentiment HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "# flask\n",
        "from flask import Flask, jsonify, request\n",
        "import jwt\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/predict_sentiment', methods=['GET'])\n",
        "def predict_sentiment_endpoint():\n",
        "\n",
        "    auth_header = request.headers.get('Authorization')\n",
        "\n",
        "    if auth_header and auth_header.startswith('Bearer '):\n",
        "        # Extract the token from the Authorization header\n",
        "        auth_token = auth_header.split(' ')[1]\n",
        "\n",
        "        # Decode the JWT token to access user information\n",
        "        decoded_token = jwt.decode(auth_token, os.getenv(\"JWT_SECRET_KEY\"), algorithms=['HS256'])\n",
        "\n",
        "        user_id = decoded_token.get('userId')\n",
        "\n",
        "        if user_id:\n",
        "            result = predict_sentiment(user_id)\n",
        "            return jsonify(result)\n",
        "        else:\n",
        "            return jsonify({'error': 'userId not found in token'}), 401\n",
        "    else:\n",
        "        return jsonify({'error': 'authToken not provided in the Authorization header'}), 401\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(host='0.0.0.0', port=8888)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNdsZze64Hk+3tv2LMLyHMk",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
